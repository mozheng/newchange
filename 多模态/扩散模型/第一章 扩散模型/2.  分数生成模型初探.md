## 2.1 分数模型相关基础
### 2.1.1 分数生成模型
第一章第一节提到了**分数生成模型（Score-based Generative Model）**。这部分主要源于NCSN （Noise Conditional Score Networks）论文[2]，它来自于宋飏博士这是宋飏发表在 NeurIPS2019 上面的文章。宋飏博士认为现有的生成模型可以大体分为两种[3]：**基于似然的模型**与**隐式生成模型**。基于似然的模型要么为似然的计算对模型结构做很强的限制，要么依赖目标函数来做“近似极大似然”的训练。而隐式生成模型要求对抗训练，模型不稳定很容易崩溃。为了规避这些问题，宋飏博士选择从大量被噪声扰动的数据分布中学习分数函数（score function），即“**stein分数**” $\nabla _{{x}}\log p({x})$ 。并使用朗之万动力学（Langevin dynamics）的方法从估计的数据分布中进行采样来生成新的样本。这样得到的生成模型通常称为“**基于分数的生成模型**”（Score-based Generative Models，SGM）。
### 2.1.2 “分数”的起源
传统生成模型的目标就是要得到数据的分布。例如一个数据集 ${x_1, x_2, ..., x_N}$ 的数据的概率密度分布（注意，这里是概率密度分布，PDF）为 $p(x)$ 。起初我们认为初始的数据是杂乱的，随机的，我们可以记为：
$$
p_{\theta}({x}) = \frac{e^{-f_{\theta}({x})}}{C_{\theta}},f_{\theta}({x})\in \mathbb{R} \tag{2.12}\\
$$
$\theta$ 是参数用于建模， $f_{\theta}$ 被称为核心能量模型（energy-based model）。这个函数型就很像高斯函数 $f(x)=\frac{1}{σ \sqrt{2π}} \cdot e^{\frac{-(x - μ)^2} {2σ^2}}$ 。我们通过最大化log似然的方式中求参数$\theta$ 。即首先计算基于x的导数为：
$$
\nabla _{{x}}\log(p_{\theta}({x})) = -\nabla _{{x}}f _{\theta}({x}) - \nabla _{{x}}\log C_{\theta} = -\nabla _{{x}}f _{\theta}({x}) \tag{2.13} \\
$$
因为C与无关，所以 $\nabla _{{x}}\log C_{\theta}=0$ 。这里突然发现，如果求解 $\nabla _{{x}}\log p({x})$ 我们就不需求解常数C了。

那么，这个分数具体有什么意义呢？从数学的角度出发来看，它是一个“矢(向)量场”(vector field) 。向量的方向是：对于输入数据(样本)来说，其对数概率密度增长最快的方向。（下图仅仅是示意，不代表真实场景）如果在采样过程中沿着分数的方向走，就能够走到数据分布的高概率密度区域（即为中心，方向近乎垂直区），最终生成的样本就会符合原数据分布。
### 2.1.3  郎之万动力学采样方法

朗之万动力学（Langevin dynamics）原是描述物理学中布朗运动（悬浮在液体或气体中的微小颗粒所做的无规则运动）的微分方程，借鉴到这里作为一种生成样本的方法。从一个分布中采样时，经常使用这种方法。概括地来说，该方法首先从先验分布随机采样一个初始样本，然后利用模型估计出来的分数逐渐将样本向数据分布的高概率密度区域靠近。为保证生成结果的多样性，我们需要采样过程带有随机性。当经过中所述的分数匹配方法训练深度生成模型后，可以使用具有迭代过程的朗之万动力学采样方法从分布中来生成新的样本。
朗之万动力学采样过程描述：假设初始数据满足先验分布 $x_0 \sim \pi(x)$ ，然后使用迭代过程
$$
\begin{align*}
x_{t+1}&=x_t+\frac{\epsilon}{2}\nabla _{x}\log  p\left( x \right) +\sqrt{\epsilon}\boldsymbol{z}_t, t=0,1,2,\cdots ,T\\
&=x_t+\frac{\epsilon}{2}s_{\theta}\left( x \right) +\sqrt{\epsilon}\boldsymbol{z}_t, t=0,1,2,\cdots ,T
\end{align*}
$$
其中， $z_i \sim \mathcal{N}(0,I)$ 为标准高斯分布，可以看成是噪声的概念。理论上，当 $k \rightarrow \infty, \epsilon \rightarrow 0$ ，最终生成的样本 $x_T$ 将会服从原数据分布 $p_{data}(x)$，趋于某一个特定值。
![](images/2.2.2.jpg)
图 2.2 从左到右，图片模拟的是使用郎之万动力学在两个高斯分布中采样，最后采样的结果很符合原始高斯分布

## 2.2 分数生成推理过程
### 2.2.1 分数生成模型公式推理
现在我们想要训练一个神经网络来估计出真实的分布。 $s_\theta({x})$ 就是这个神经网络的分数，同理 $\theta$ 还是代表网络参数。我们可以最小化真实的score function，用这种形似来优化即可。
$$\mathcal{L} = \mathbb{E}_{p({x})}[||\nabla _{{x}}\log p({x}) - {s} _{\theta}({x})||^{2}] \tag{2.14}$$
但是这样的一个loss我们是算不出来的，因为我们并不知道真实的$p({x})$是什么。我们把上面loss的期望根据上一节公式（1.7）写开，同时二次范式项打开，可以得到
$$
\begin{align*}\mathcal{L} =& \mathbb{E}_{p({x})}[||\nabla _{{x}}\log p({x}) - {s} _{\theta}({x})||^{2}]\\
=& \int p({x}) [||\nabla _{{x}}\log p({x})||^{2} + ||{s} _{\theta}({x})||^{2} - 2(\nabla _{{x}}\log p({x}))^{T}{s} _{\theta}({x})] d {x}\end{align*}\\
$$
第一项对于 $\theta$ 来说是常数可以忽略。因为我们要算网络 $\theta$ 的参数，与 $x$ 无关，我们可以把第一项去掉，不予考虑。
第二项为：
$$\int p({x}) ||{s} _{\theta}({x})||^{2} d {x}$$
对于第三项，若x的维度为N则有：
$$
\begin{align*}
& -2\int p({x}) (\nabla _{{x}}\log p({x}))^{T}{s} _{\theta}({x}) d {x}\\ 
=& -2 \int p({x}) \sum\limits_{i=1}^{N}(\frac{\partial \log p({x})}{\partial {x}_{i}}{s}_{\theta_i}({x})) d {x}\\ 
=& -2 \sum\limits_{i=1}^{N} \int p({x}) \frac{1}{p({x})} \frac{\partial p({x})}{\partial {x}_{i}}{s}_{\theta_i}({x}) d {x}\\ 
=& -2 \sum\limits_{i=1}^{N} \int \frac{\partial p({x})}{\partial {x}_{i}}{s}_{\theta_i}({x}) d {x}\\ 
=& 2 \sum\limits_{i=1}^{N} - \int( \frac{\partial (p({x}){s}_{\theta_i}({x}))}{\partial {x}_{i}} d {x} + \int p({x}) \frac{\partial {s}_{\theta_i}({x})}{\partial {x}_{i}}) d {x}
\end{align*}
$$
上面的最后一步是分段求积分公式，因为 $p(x)$ 取极限是0，这是PDF的特性。可得下面左边那部分为0。
$$
\begin{align*}
=& 2 - (p({x}){s}_{\theta_i}({x})\bigg\rvert^{\infty}_{-\infty}) + \sum\limits_{i=1}^{N}\int p({x}) \frac{\partial {s}_{\theta i}({x})}{\partial {x}_{i}} d {x}\\ 
=& 2 \sum\limits_{i=1}^{N} \int p({x}) \frac{\partial {s}_{\theta i}({x})}{\partial {x}_{i}} d {x}\\ 
=& 2\int p({x}) \sum\limits_{i=1}^{N} \frac{\partial {s}_{\theta i}({x})}{\partial {x}_{i}} d {x}\\ 
=& 2\int p({x}) \text{tr}(\nabla _{{x}}{s}_{\theta}({x})) d {x}
\end{align*}
$$
$tr(.)$ 函数表示矩阵的迹(trace)，即矩阵主对角线元素的总和。因为 $\nabla _{{x}}{s}_{\theta}({x})$ 是个二阶海森矩阵求导，这里只用主对角线就行。
所以最后的loss是第二和第三项的和，因为方便操作，论文中的Loss公式乘上了1/2，结果不影响：
$$\begin{align*} \frac{1}{2}\mathcal{L} &= \frac{1}{2}\int p({x}) ||{s} _{\theta}({x})||^{2} d {x} + \int p({x}) \text{tr}(\nabla _{{x}}{s}_{\theta}({x})) d {x}\\\\ &= \mathbb{E}_{p({x})}[||\frac{1}{2}{s} _{\theta}({x})||^{2} + \text{tr}(\nabla _{{x}}{s}_{\theta}({x}))] \tag{2.15} \end{align*}$$
公式似乎越来越复杂 $tr(.)$ 到底是怎么求？作者在噪声条件分数网络 NCSN 论文[3]中给出了一种解决方案。具体做法就是对原始数据加噪，使得其结果满足我们预先定义好的分布，比如高斯分布。这样，我们就知道了现在的概率密度了，于是就可以进行训练。具体操作我们接下来介绍。
### 2.2.2  噪声条件分数网络（Noise Conditional Score Networks，NCSN）
去噪分数匹配（Denoising Score Matching，DSM）方法是作者在噪声条件分数网络 NCSN 中默认的计算方法[3]。当前的问题是：不知道**原始数据分布的梯度向量**。有一种解题方法是从标准高斯噪声中构建新分布。我们考虑从标准高斯分布 $\mathcal{N}(0,𝐼)$ 中采样随机噪声𝜖，然后乘上我们预定义的 $𝜎$ ，接着加到样本 $𝑥$ 中，从而就能得到加噪后的样本 $\tilde{𝑥}$。我们构建新分布为：
$$\begin{align}
q_{\sigma}(\tilde{x}) &=\int q_σ(\tilde{x}|x) p_{data}(x) dx \tag{2.16} \\
& = \int \mathcal{N}(\tilde{x}|x,σ^2I)p_{data}(x) dx \tag{2.16+}\\
\text{and} ~ \tilde{x}&=x+\sigma\epsilon, ~ \epsilon\sim \mathcal{N}(0,I) \tag{2.17} \\
\end{align}$$公式（2.16）的积分是代表采样，有些地方会看到公式（2.16+）的形式。$p_{data}(x)$ 代表需要扰动的数据。$q_σ(\tilde{x}|x) \sim \mathcal{N}(\tilde{x}|x,σ^2I)$ 表示扰动噪声数据分布。$q_{\sigma}(\tilde{x})$ 就是我们构建的新分布， $\tilde{𝑥}$就是我们加噪的新样本。
NCSN的目标是训练一个条件分数网络 $s_{\theta}( \tilde{x},\sigma )$ 来估计扰动数据的分布，即 
$$
\frac{1}{2}\mathbb{E}_{q_σ(\tilde{x})}
\Big[\Vert s_{\theta}( \tilde{x},\sigma ) - \nabla _{\tilde{x}} \log q_σ(\tilde{x})\Vert_2^2\Big] \tag{2.18}
$$
在DSM论文[4]中，我们得到相应的推导公式
$$
\begin{align}
& ~~~~ \frac{1}{2}\mathbb{E}_{q_σ(\tilde{x})}
\Big[\Vert s_{\theta}( \tilde{x},\sigma ) - \nabla _{\tilde{x}} \log q_σ(\tilde{x})\Vert_2^2\Big] \tag{2.19} \\
&=\frac{1}{2}\int {q_σ(\tilde{x})}
\Big[\Vert s_{\theta}( \tilde{x},\sigma ) - \frac{1}{q_σ(\tilde{x})}\frac{\partial q_σ(\tilde{x})}{\partial \tilde{x}} \Vert_2^2\Big] d\tilde{x}\\
&=\frac{1}{2}\int 
\Big[\Vert {q_σ(\tilde{x})}s_{\theta}( \tilde{x},\sigma ) - \frac{\partial q_σ(\tilde{x})}{\partial \tilde{x}} \Vert_2^2\Big] d\tilde{x}\\
&=\frac{1}{2}\int 
\Big[\Vert {q_σ(\tilde{x})}s_{\theta}( \tilde{x},\sigma ) - \frac{\partial \int q_σ(\tilde{x}|x) p_{data}(x) dx}{\partial \tilde{x}} \Vert_2^2\Big] d\tilde{x} \\
&=\frac{1}{2}\int 
\Big[\Vert {q_σ(\tilde{x})}s_{\theta}( \tilde{x},\sigma ) - \int \frac{\partial q_σ(\tilde{x}|x)}{\partial \tilde{x}} p_{data}(x) dx \Vert_2^2\Big] d\tilde{x} \\
&=\frac{1}{2}\int 
\Big[\Vert {\int q_σ(\tilde{x}|x) p_{data}(x) dx} \cdot s_{\theta}( \tilde{x},\sigma ) - \int q_σ(\tilde{x}|x) p_{data}(x)\frac{\partial \log q_σ(\tilde{x}|x)}{\partial \tilde{x}} dx \Vert_2^2\Big] d\tilde{x} \\
&=\frac{1}{2}\int_{\tilde{x}} \int_x q_σ(\tilde{x}|x) p_{data}(x) 
\Big[\Vert s_{\theta}( \tilde{x},\sigma ) - \frac{\partial \log q_σ(\tilde{x}|x)}{\partial \tilde{x}}\Vert_2^2\Big] dx d\tilde{x} \\
&= \frac{1}{2}\mathbb{E}_{q_σ(\tilde{x}|x) p_{data}(x) }
\Big[\Vert s_{\theta}( \tilde{x},\sigma ) - \nabla _{\tilde{x}} \log q_σ(\tilde{x}|x)\Vert_2^2\Big] \tag{2.20}
\end{align}
$$
***
注意：宋飏博士在论文与blog有不同的标记方法。论文更准确用的是 $q_{\sigma}(x)$ ，代表与 $p(x)$ 不同；blog上用的是 $p_{\sigma}(x)$ ，为了从公式上直接替换 $p(x)$ 。
***
这里面还有一个潜在推理。当 $x$ 是 $D$ 维，即 ${x}\in \mathbb{R} ^D$ 时，由多维高斯分布 $f( \mathbf{x} ) = \frac{1}{(2\pi)^{\frac{D}{2}} | Σ |^{\frac{1}{2}}} e^{- \frac{1}{2}( x - μ )^T Σ^{-1}(x - μ)}$  可得:
$$
\begin{align}
\nabla _{\tilde{x}} \log q_σ(\tilde{x}|x) &= \nabla _{\tilde{x}}(C - \frac{1}{2}(\tilde{x} - x )^T Σ^{-1}(\tilde{x} - x)) \\
&= -Σ^{-1}(\tilde{x} - x) \\
&= -\begin{bmatrix} 𝜎_{1}^2 & & & \\ & 𝜎_{2}^2 & & \\ & & \ddots &\\ & & &𝜎_{d}^2 \end{bmatrix} (\tilde{x} - x) \tag{2.21}\\
&= - \frac{\tilde{x} - x}{𝜎^2} \tag{2.22} \\
&= - \frac{\epsilon}{𝜎} \tag{2.23}
\end{align}
$$
将（2.22）带入公式（2.20）可以写为如下形式：
$$
\begin{align}
& ~~~~ \frac{1}{2}\mathbb{E}_{q_σ(\tilde{x}|x) p_{data}(x) }
\Big[\Vert s_{\theta}( \tilde{x},\sigma ) - \nabla _{\tilde{x}} \log q_σ(\tilde{x}|x)\Vert_2^2\Big] \\
&= \frac{1}{2}\mathbb{E}_{q_σ(\tilde{x}|x)}\mathbb{E}_{p_{data}(x)}
\Big[\Big\Vert s_{\theta}( \tilde{x},\sigma ) + \frac{\tilde{x}-x}{σ^2}\Big\Vert_2^2\Big] \\ 
&= \frac{1}{2}\mathbb{E}_{p_{data}(x)}\mathbb{E}_{\tilde{x} \sim \mathcal{N}(\tilde{x}|x,σ^2I)}
\Big[\Big\Vert s_{\theta}( \tilde{x},\sigma ) + \frac{\tilde{x}-x}{σ^2}\Big\Vert_2^2\Big] \tag{2.24} \\
& =\mathcal{l}(\theta;\sigma) \tag{2.25}
\end{align}
$$
我们把最后公式（2.24）的写成公式（2.25）方便后面使用。在公式（2.24）中，我们发现，主要的问题在于噪声力度 $\sigma$ 。噪声加的大（极端情况变成纯噪声）会导致加噪之后的数据分布严重偏离原始数据分布；反之，噪声加的小（极端情况变成0噪声）不能很好的解决前述存在的问题。为了权衡这两个方面，作者提出同时使用多个不同大小噪声力度的退火方案。
### 2.2.6 退火朗之万动力学采样（annealed Langevin dynamics）

有 $L$ 个递增的 $𝜎_1<𝜎_2<⋯<𝜎_L$ ，分布 $q(X)$ 与每个高斯噪声 $\sigma\epsilon \sim \mathcal{N}(0,𝜎_i^2I),i=1,2,⋯,L$ 结合成受扰动的分布。我们把公式（

我们发现 $\sigma$ 最够小时， $\tilde{x} \approx x$  ，。

，其中当 ${x}\in \mathbb{R} ^D$ 时 $s_{\theta}\left( {x},\sigma \right) \in \mathbb{R} ^D$ 。一般把 $s_{\theta}\left( {x};t \right)$ 称为噪声条件分数网络。


![](分数在高低区域的作用.png)
我们再回到向量场的图，这个图仅仅演示用。真实情况是图像大部位是的区域是低概率密度区域，而损失函数简单的二阶计算会出现大密度区间“妨碍”小密度区间的问题。这种情况我们会加大噪声，填充整个区域。较大的噪声显然可以覆盖低密度区域，在更多区域获得更好的评分估计，但是过度破坏了数据。如图我们可以看到边界模糊，这就是强度过大的噪声反之会干扰到原始数据的分布的表现。这种情况还会造成估计分数的误差增大，从而基于加噪后的分数使用朗之万动力学采样生成的结果也就不太符合原数据分布。相反地，噪声强度小能够获得与原数据分布较为近似的效果，但是却不能够很好地“填充”低概率密度区域。为了达到两者最佳。我们使用了多尺度噪声扰动。






[1] Jonathan Ho, Ajay Jain, Pieter Abbeel. Denoising Diffusion Probabilistic Models. arXiv preprint  arXiv:2006.11239v2.
[2] Yang Song, Jascha Sohl-Dickstein, Diederik P. Kingma, Abhishek Kumar, Stefano Ermon, Ben Poole. Score-Based Generative Modeling through Stochastic Differential Equations. arXiv:2011.13456
[3] Yang Song, and Stefano Ermon. Generative Modeling by Estimating Gradients of the Data Distribution. arXiv:1907.05600
[4] Pascal Vincent. A Connection Between Score Matching and Denoising Autoencoders