在深入探讨扩散模型的理论基础之后，本章将聚焦于似然模型和降噪扩散模型（Denoising Diffusion Probabilistic Models，DDPM），以及基于分数模型的噪声控制分数方程（Noise Conditional Score Networks，NCSN）的联系。我们将从第一章的基础知识出发，进一步揭示这些模型如何在生成模型领域内提供强大的理论支持和实际应用。
## 2.1 似然模型与DDPM

似然模型在统计学和机器学习中扮演着核心角色，它们通过最大化观测数据的似然函数来估计模型参数。在第一章第1节中，我们得知扩散模型来源于物理学的扩散过程。在生成模型中，它演化为从高斯纯噪声数据逐渐对数据进行去噪的过程。如果从单个图像过程的观点来讨论，扩散过程是不断往图像上加噪声直到图像变成一个纯噪声，扩散过程从开头到最后是一个马尔可夫链，该过程由 $q_\phi(x_t|x_{t-1})$ 来标记。又因为VDM第二条约束规则，因为均值和方差是预先设定的高斯分布，该过程不再由参数 $\phi$ 参数化，故记为 $q(x_t|x_{t-1})$ 。逆扩散过程是从纯噪声生成一张图像的过程，该过程用 $p_\theta(x_{t-1}|x_t)$ 来标记。整体过程如下图2.1 。
![](images/2.1.1.jpg)
图2.1 扩散过程

我们使用第一章第1节的扩散模型基础进行推论，并结合原始实现代码DDPM，理论联合实际进行详细讲解。本文主要部分来自《Denoising Diffusion Probabilistic Models》[1]
### 2.1.1 DDPM前向过程-混入噪声
在推理之前，我们先重点回顾一下上一节的内容。
1. $x_0$ 为原始图像，$x_t$ 为加t步噪声后的图像，噪声加到最后（第T步）为 $x_T$ 
2. 图像加噪声为 $q(x_t|x_{t-1})$ 操作，意为从 $x_{t-1}$ 加噪声成 $x_t$ ；图像降噪为 $p_\theta(x_{t-1}|x_t)$ 操作，意为从 $x_t$ 降噪声成 $x_{t-1}$ 
3. 每一时间步加噪声 $\epsilon_t$（或去噪声）都有参数对 $\alpha_t$ ，其中有 $0<\beta_t<<\alpha_t<1$ ，且 $\beta_t+\alpha_t=1$ 。DDPM多一个 $\beta_t$ 参数，为了方便
那么，图像加噪过程就很直接，根据第一章第一节的公式（63）
$$
\begin{align}
x_t = \sqrt{\alpha_t}x_{t-1} + \sqrt{1-\alpha_t}\epsilon_t  \tag{2.1} \\
\text{with}: \epsilon_t \sim \mathcal{N}(\epsilon;0,I) 
\end{align}
$$
同理，经过公式（64-73）的计算推理，可以得到：
$$
\begin{align}
x_t&=\sqrt{\alpha_t}x_{t-1}+\sqrt{1-\alpha_t}\epsilon_{t-1}^*  \\
&=\sqrt{\bar{\alpha}_t}x_0 + \sqrt{1-\bar{\alpha}_t}\epsilon_0  \tag{2.2} \\
q(x_t)&=q(x_t|x_0)\sim \mathcal{N}(x_t;\sqrt{\bar{\alpha}_t}x_0,(1-\bar{\alpha}_t)I) \tag{2.3}
\end{align}
$$
这一段对应的python的代码也很直接。其中 (sqrt_alphas_cumprod, t) 就是 $\sqrt{\bar{\alpha}_t}$ ，(sqrt_one_minus_alphas_cumprod, t) 就是 $\sqrt{1-\bar{\alpha}_t}$ 。
```python
def q_sample(self, x_start, t, noise=None):
	noise = default(noise, lambda: torch.randn_like(x_start))
	return (
		extract(self.sqrt_alphas_cumprod, t, x_start.shape) * x_start +
		extract(self.sqrt_one_minus_alphas_cumprod, t, x_start.shape) * noise
	)
```

### 2.1.2 DDPM后向过程
回顾第一章第1节的VDM的通用ELBO推导公式（51-62），
$$
\begin{align}
\log p(x)&\geq  \mathbb{E}_{q(x_{1:T}|x_0)} \Big[\log{\frac{p(x_{0:T})}{q(x_{1:T}|x_0)}} \Big] \\
&= \underbrace{\mathbb{E}_{q(x_{1}|x_0)} [ \log p_\theta(x_0 | x_1)]}_{重建项} -   \underbrace{D_{KL}(q(x_T|x_0)||p(x_T))}_{先验匹配项} - \underbrace{\sum_{t=2}^{T} \mathbb{E}_{q(x_t|x_0)} [D_{KL}(q(x_{t-1}|x_t,x_0)||p_\theta(x_{t-1}|x_t))]}_{去噪匹配项} \tag{2.4}
\end{align}
$$
我们知道 $\mathbb{E}_{q(x_t|x_0)} [D_{KL}(q(x_{t-1}|x_t,x_0)||p_\theta(x_{t-1}|x_t))]$ 是与时间 $t$ 直接有关的去噪匹配项，计算程度直接决定了ELBO的计算程度。我们希望尽可能地将**近似去噪分布** $p_θ(x_{t−1}|x_t)$ 与**真值去噪分布** $q(x_{t−1}|x_t,x_0)$ 相匹配，才能做好生成模型的预测。同时 在VDM中，由第一章第1节的公式（74-87）我们知道：
$$
q(x_{t−1}|x_t,x_0)\propto \mathcal{N}({{x}}_{t-1};\underbrace{\frac{\sqrt{{\alpha}_t}(1-\bar{{\alpha}}_{t-1}){{x}}_t+\sqrt{\bar{{\alpha}}_{t-1}}(1-{\alpha}_t){{x}}_{0}}{1-\bar{{\alpha}}_{t}}}_{{{\mu}}_q({{x}}_t,{{x}}_0)},\underbrace{\frac{(1-{\alpha}_t)(1-\bar{{\alpha}}_{t-1})}{1-\bar{{\alpha}}_{t}} {{I}}}_{{{\Sigma}}_{q}(t)}) \tag{2.5}
$$
我们注意到均值，方差可写做：
$$
\begin{align}
\mu_q(x_t,x_0)&=\frac{\sqrt{{\alpha}_t}(1-\bar{{\alpha}}_{t-1}){{x}}_t+\sqrt{\bar{{\alpha}}_{t-1}}(1-{\alpha}_t){{x}}_{0}}{1-\bar{{\alpha}}_{t}}  \tag{2.6} \\
σ_q^2(t)&=\frac{(1-{\alpha}_t)(1-\bar{{\alpha}}_{t-1})}{1-\bar{{\alpha}}_{t}} \tag{2.7}
\end{align} 
$$
因为DDPM用的是噪声推导模式，将如下公式（105） $x_0 = \frac{x_t-\sqrt{1-\bar{\alpha}_t}\epsilon_0}{\sqrt{\bar{\alpha}_t}}$ 带入均值可得
$$
\begin{align}
\mu_q(x_t,x_0)&=\frac{\sqrt{{\alpha}_t}(1-\bar{{\alpha}}_{t-1}){{x}}_t+\sqrt{\bar{{\alpha}}_{t-1}}(1-{\alpha}_t)\frac{x_t-\sqrt{1-\bar{\alpha}_t}\epsilon_0}{\sqrt{\bar{\alpha}_t}}}{1-\bar{{\alpha}}_{t}} \\
&= \frac{1}{\sqrt{\alpha_t}}x_t - \frac{1-{\alpha}_t}{\sqrt{1-\bar{{\alpha}}_t}\sqrt{\alpha_t}}\epsilon_0 \tag{2.8}
\end{align}
$$
根据噪声预测的优化策略，我们有
$$
\begin{align}
&~~~~\arg\min_{{{\theta}}} D_{\text{KL}}(q({{x}}_{t-1}|{{x}}_t,{{x}}_0)\Vert p_{{\theta}}({{x}}_{t-1}|{{x}}_t))\\
&=\arg\min_{{{\theta}}} \frac{1}{2σ_q^2 (t)} \frac{(1-{\alpha}_t)^2}{(1-\bar{{\alpha}}_t)\alpha_t} \left[ \Big\Vert (\epsilon_0 - \hat{\epsilon}_\theta(x_t,t)) \Big\Vert_2^2 \right] \\
&=\arg\min_{{{\theta}}} \frac{1}{2σ_q^2 (t)} \frac{(1-{\alpha}_t)^2}{(1-\bar{{\alpha}}_t)\alpha_t} \left[ \Big\Vert (\epsilon_0 - \hat{\epsilon}_\theta(\sqrt{\bar{\alpha}_t}x_0+\sqrt{1-\bar{\alpha}_t}\epsilon_t,t)) \Big\Vert_2^2 \right] \tag{2.9} \\
\end{align}
$$
此时我们发现只要通过拉进 $\epsilon_t$ 与 $\epsilon_\theta(x_t,t)$ 的距离的方式就可以训练参数 $\theta$。所以，我们利用噪声间MSE Loss 即可完成优化损失函数的设计：
$$ \mathcal{Loss} =  ||\epsilon_t-\epsilon_\theta(\sqrt{\bar{\alpha}_t}x_0+\sqrt{1-\bar{\alpha}_t}\epsilon_t,t)||^2 \tag{2.10}
$$
### 2.1.3 训练过程
| 训练过程伪代码                                                                                                                                                  |
| -------------------------------------------------------------------------------------------------------------------------------------------------------- |
| 1: repeat                                                                                                                                                |
| 2:    $x_0 \sim q(x_0),t\sim \text{Uniform}(\{1,2,...,T\}),\epsilon \sim \mathcal{N}(0,I)$                                                               |
| 3:    使用梯度下降逐步优化 $\nabla_\theta \left\|\epsilon_t-\epsilon_\theta\left(\sqrt{\bar{\alpha}_t} x_0+\sqrt{1-\bar{\alpha}_t} \epsilon_t, t\right)\right\|^2$ |
| 4: until 收敛                                                                                                                                              |
表2.1 训练过程伪代码
上表2.1为原论文的训练过程，该过程比较简单：
1.  循环直到收敛
	1. 从数据集中选取 $x_0$ ，这就是原始图片；随机选取时间戳 t，它代表扩散模型需要扩散的轮数；生成t个高斯噪声，每个都是 $\epsilon_t\in\mathcal{N}(0, \mathbf{I})$
	2. 调用模型 $ϵ_θ$（这里是UNet网络）预估 $\epsilon_\theta\left(\sqrt{\bar{\alpha}_t} x_0+\sqrt{1-\bar{\alpha}_t} \epsilon_t, t\right)$
	3. 计算噪声之间的 MSE Loss: $\mathcal{Loss} =  \left\|\epsilon_t-\epsilon_\theta\left(\sqrt{\bar{\alpha}_t} x_0+\sqrt{1-\bar{\alpha}_t} \epsilon_t, t\right)\right\|^2$ 并梯度下降优化UNet网络。
对应python 训练代码如下：
```python
class GaussianDiffusionTrainer(nn.Module):
    def __init__(self, model, beta_1, beta_T, T):
        super().__init__()

        self.model = model # Unet网络
        self.T = T

        self.register_buffer(
            'betas', torch.linspace(beta_1, beta_T, T).double())
        alphas = 1. - self.betas
        alphas_bar = torch.cumprod(alphas, dim=0)

        # calculations for diffusion q(x_t | x_{t-1}) and others
        self.register_buffer(
            'sqrt_alphas_bar', torch.sqrt(alphas_bar))
        self.register_buffer(
            'sqrt_one_minus_alphas_bar', torch.sqrt(1. - alphas_bar))

    def forward(self, x_0):
        """
        Algorithm 1.
        """
        t = torch.randint(self.T, size=(x_0.shape[0], ), device=x_0.device)
        noise = torch.randn_like(x_0)
        x_t = (
            extract(self.sqrt_alphas_bar, t, x_0.shape) * x_0 +
            extract(self.sqrt_one_minus_alphas_bar, t, x_0.shape) * noise)
        loss = F.mse_loss(self.model(x_t, t), noise, reduction='none')
        return loss
```
### 2.1.4 推理过程
DDPM的公式是根据（2.5，2.8）的公式更改了一下。
$$
\begin{align}
q(x_{t−1}|x_t,x_0) &\propto \mathcal{N}({{x}}_{t-1};\frac{\sqrt{{\alpha}_t}(1-\bar{{\alpha}}_{t-1}){{x}}_t+\sqrt{\bar{{\alpha}}_{t-1}}(1-{\alpha}_t){{x}}_{0}}{1-\bar{{\alpha}}_{t}},\frac{(1-{\alpha}_t)(1-\bar{{\alpha}}_{t-1})}{1-\bar{{\alpha}}_{t}} {{I}})  \\
&=\mathcal{N}({{x}}_{t-1};\frac{1}{\sqrt{\alpha_t}}(x_t - \frac{1-{\alpha}_t}{\sqrt{1-\bar{{\alpha}}_t}}\epsilon_0),\frac{(1-{\alpha}_t)(1-\bar{{\alpha}}_{t-1})}{1-\bar{{\alpha}}_{t}} {{I}})  \\
\end{align}
$$
此时，在我们再次强调一下我们在第一章提到的核心目标：
用未知 $x_0$ 原始值的**近似去噪分布** $p_θ(x_{t−1}|x_t)$ 来近似已知原始图片 $x_0$ 的**真值去噪分布** $q(x_{t−1}|x_t,x_0)$ 
那么 $p_θ(x_{t−1}|x_t)$ 可以尽可能的仿作：
$$
p_\theta({x}_{t-1} \vert {x}_t) = \mathcal{N}(\frac{1}{\sqrt{\alpha_t}}(x_t-\frac{1-\alpha_t}{\sqrt{1-\bar{\alpha}_t}} \epsilon_\theta(x_t,t)),\frac{(1-{\alpha}_t)(1-\bar{{\alpha}}_{t-1})}{1-\bar{{\alpha}}_{t}}I)
$$
我们知道 $x_{t-1} \sim p_\theta(x_{t-1}|x_t)$ ，按照初中的规则，我们可以将其化为正态分布的格式，步骤如下：
$$
\begin{align}
& \frac{x_{t-1} - \mu}{\sigma} \sim \mathcal{N}(0,I)=z \\
& x_{t-1} = \mu + \sigma z \\
& x_{t-1} =\frac{1}{\sqrt{\alpha_t}}(x_t-\frac{1-\alpha_t}{\sqrt{1-\bar{\alpha}_t}} \epsilon_\theta(x_t,t)) + \frac{(1-\alpha)(1 - \bar{\alpha}_{t-1})}{1 - \bar{\alpha}_t}  z \tag{2.11}
\end{align}
$$
在DDPM中，用一个新的符号 $z$ 代替标准正态分布噪声，虽然本质与 $\epsilon$ 相同，但实际含义不一样。伪代码如表2.2所示：

| 前向推理采样算法伪代码                                                                                                                      |
| -------------------------------------------------------------------------------------------------------------------------------- |
| 1: $x_T \sim \mathcal{N}(0,I)$                                                                                                   |
| 2: for $t=T,...,1$ do:                                                                                                           |
| 3:     $z \sim \mathcal{N}(0,I)$ if $t>1$ ， else $z=0$                                                                           |
| 4:     $x_{t-1} =\frac{1}{\sqrt{\alpha_t}}(x_t-\frac{1- \alpha_t}{\sqrt{1-\bar{\alpha}_t}} \epsilon_\theta(x_t,t)) + \sigma_t z$ |
| 5: end for                                                                                                                       |
| 6: return $x_0$                                                                                                                  |
表2.2 采样算法伪代码

此时已经训练出来了 $\epsilon_θ$ （这里是UNet网络），所以在下面的推理过程中 $ϵ_θ(x_t,t)$ 是已知的。假设我用推理的过程中扩散T步，那么从T步开始逆向回推，每一步有如下操作：
1. 初始化最终的扩散状态 $x_T$ 为纯高斯噪声，从这个状态开始进行反推。
2. 从 $t=T$ 步开始，每步减一，直到 $t=1$ ：
	1. 如果是最后一轮循环 $t=1$ ，噪声 $z = 0$ ；如果 $t > 1$ 时，即可取随机噪声 $z\in\mathcal{N}(0, \mathbf{I})$ 。
	2. 因为公式（2.11）推论 $x_{t-1} =\frac{1}{\sqrt{\alpha_t}}(x_t-\frac{1-\alpha_t}{\sqrt{1-\bar{\alpha}_t}} \epsilon_\theta(x_t,t)) + \frac{(1-\alpha)(1 - \bar{\alpha}_{t-1})}{1 - \bar{\alpha}_t}  z$

3. 最后一步返回 $x_0$
对应的python代码如下，同样很清晰：
```python
class GaussianDiffusionSampler(nn.Module):
    def __init__(self, model, beta_1, beta_T, T):
        super().__init__()

        self.model = model # Unet
        self.T = T

        self.register_buffer('betas', torch.linspace(beta_1, beta_T, T).double())
        alphas = 1. - self.betas
        alphas_bar = torch.cumprod(alphas, dim=0)
        alphas_bar_prev = F.pad(alphas_bar, [1, 0], value=1)[:T]

        self.register_buffer('coeff1', torch.sqrt(1. / alphas))
        self.register_buffer('coeff2', self.coeff1 * (1. - alphas) / torch.sqrt(1. - alphas_bar))

        self.register_buffer('posterior_var', self.betas * (1. - alphas_bar_prev) / (1. - alphas_bar))

    def predict_xt_prev_mean_from_eps(self, x_t, t, eps):
        assert x_t.shape == eps.shape
        return (
            extract(self.coeff1, t, x_t.shape) * x_t -
            extract(self.coeff2, t, x_t.shape) * eps
        )

    def p_mean_variance(self, x_t, t):
        # below: only log_variance is used in the KL computations
        var = torch.cat([self.posterior_var[1:2], self.betas[1:]])
        var = extract(var, t, x_t.shape)

        eps = self.model(x_t, t)
        xt_prev_mean = self.predict_xt_prev_mean_from_eps(x_t, t, eps=eps)

        return xt_prev_mean, var

    def forward(self, x_T):
        """
        Algorithm 2.
        """
        x_t = x_T
        for time_step in reversed(range(self.T)):
            print(time_step)
            t = x_t.new_ones([x_T.shape[0], ], dtype=torch.long) * time_step
            mean, var= self.p_mean_variance(x_t=x_t, t=t)
            # no noise when t == 0
            if time_step > 0:
                noise = torch.randn_like(x_t)
            else:
                noise = 0
            x_t = mean + torch.sqrt(var) * noise # μ+σ*z
            assert torch.isnan(x_t).int().sum() == 0, "nan in tensor."
        x_0 = x_t
        return torch.clip(x_0, -1, 1)
```

## 2.2 分数模型与NCSN
### 2.2.1 分数生成模型
第一章第一节提到了**分数生成模型（Score-based Generative Model）**。这部分主要源于NCSN （Noise Conditional Score Networks）论文[2]，它来自于宋飏博士这是宋飏发表在 NeurIPS2019 上面的文章。宋飏博士认为现有的生成模型可以大体分为两种[3]：**基于似然的模型**与**隐式生成模型**。基于似然的模型要么为似然的计算对模型结构做很强的限制，要么依赖目标函数来做“近似极大似然”的训练。而隐式生成模型要求对抗训练，模型不稳定很容易崩溃。为了规避这些问题，宋飏博士选择从大量被噪声扰动的数据分布中学习分数函数（score function），即“**stein分数**” $\nabla _{{x}}\log p({x})$ 。并使用朗之万动力学（Langevin dynamics）的方法从估计的数据分布中进行采样来生成新的样本。这样得到的生成模型通常称为“**基于分数的生成模型**”（Score-based Generative Models，SGM）。
### 2.2.2 “分数”的起源
传统生成模型的目标就是要得到数据的分布。例如一个数据集 ${x_1, x_2, ..., x_N}$ 的数据的概率密度分布（注意，这里是概率密度分布，PDF）为 $p(x)$ 。起初我们认为初始的数据是杂乱的，随机的，我们可以记为：
$$
p_{\theta}({x}) = \frac{e^{-f_{\theta}({x})}}{C_{\theta}},f_{\theta}({x})\in \mathbb{R} \tag{2.12}\\
$$
$\theta$ 是参数用于建模， $f_{\theta}$ 被称为核心能量模型（energy-based model）。这个函数型就很像高斯函数 $f(x)=\frac{1}{σ \sqrt{2π}} \cdot e^{\frac{-(x - μ)^2} {2σ^2}}$ 。我们通过最大化log似然的方式中求参数$\theta$ 。即首先计算基于x的导数为：
$$
\nabla _{{x}}\log(p_{\theta}({x})) = -\nabla _{{x}}f _{\theta}({x}) - \nabla _{{x}}\log C_{\theta} = -\nabla _{{x}}f _{\theta}({x}) \tag{2.13} \\
$$
因为C与无关，所以 $\nabla _{{x}}\log C_{\theta}=0$ 。这里突然发现，如果求解 $\nabla _{{x}}\log p({x})$ 我们就不需求解常数C了。

那么，这个分数具体有什么意义呢？从数学的角度出发来看，它是一个“矢(向)量场”(vector field) 。向量的方向是：对于输入数据(样本)来说，其对数概率密度增长最快的方向。（下图仅仅是示意，不代表真实场景）如果在采样过程中沿着分数的方向走，就能够走到数据分布的高概率密度区域（即为中心，方向近乎垂直区），最终生成的样本就会符合原数据分布。
### 2.2.3  郎之万动力学采样方法

朗之万动力学（Langevin dynamics）原是描述物理学中布朗运动（悬浮在液体或气体中的微小颗粒所做的无规则运动）的微分方程，借鉴到这里作为一种生成样本的方法。从一个分布中采样时，经常使用这种方法。概括地来说，该方法首先从先验分布随机采样一个初始样本，然后利用模型估计出来的分数逐渐将样本向数据分布的高概率密度区域靠近。为保证生成结果的多样性，我们需要采样过程带有随机性。当经过中所述的分数匹配方法训练深度生成模型后，可以使用具有迭代过程的朗之万动力学采样方法从分布中来生成新的样本。
朗之万动力学采样过程描述：假设初始数据满足先验分布 $x_0 \sim \pi(x)$ ，然后使用迭代过程
$$
\begin{align*}
x_{t+1}&=x_t+\frac{\epsilon}{2}\nabla _{x}\log  p\left( x \right) +\sqrt{\epsilon}\boldsymbol{z}_t, t=0,1,2,\cdots ,T\\
&=x_t+\frac{\epsilon}{2}s_{\theta}\left( x \right) +\sqrt{\epsilon}\boldsymbol{z}_t, t=0,1,2,\cdots ,T
\end{align*}
$$
其中， $z_i \sim \mathcal{N}(0,I)$ 为标准高斯分布，可以看成是噪声的概念。理论上，当 $k \rightarrow \infty, \epsilon \rightarrow 0$ ，最终生成的样本 $x_T$ 将会服从原数据分布 $p_{data}(x)$，趋于某一个特定值。
![](images/2.2.2.jpg)
图 2.2 从左到右，图片模拟的是使用郎之万动力学在两个高斯分布中采样，最后采样的结果很符合原始高斯分布
### 2.2.4 分数生成模型公式推理
现在我们想要训练一个神经网络来估计出真实的分布。 $s_\theta({x})$ 就是这个神经网络的分数，同理 $\theta$ 还是代表网络参数。我们可以最小化真实的score function，用这种形似来优化即可。
$$\mathcal{L} = \mathbb{E}_{p({x})}[||\nabla _{{x}}\log p({x}) - {s} _{\theta}({x})||^{2}] \tag{2.14}$$
但是这样的一个loss我们是算不出来的，因为我们并不知道真实的$p({x})$是什么。我们把上面loss的期望根据上一节公式（1.7）写开，同时二次范式项打开，可以得到
$$
\begin{align*}\mathcal{L} =& \mathbb{E}_{p({x})}[||\nabla _{{x}}\log p({x}) - {s} _{\theta}({x})||^{2}]\\
=& \int p({x}) [||\nabla _{{x}}\log p({x})||^{2} + ||{s} _{\theta}({x})||^{2} - 2(\nabla _{{x}}\log p({x}))^{T}{s} _{\theta}({x})] d {x}\end{align*}\\
$$
第一项对于 $\theta$ 来说是常数可以忽略。因为我们要算网络 $\theta$ 的参数，与 $x$ 无关，我们可以把第一项去掉，不予考虑。
第二项为：
$$\int p({x}) ||{s} _{\theta}({x})||^{2} d {x}$$
对于第三项，若x的维度为N则有：
$$
\begin{align*}
& -2\int p({x}) (\nabla _{{x}}\log p({x}))^{T}{s} _{\theta}({x}) d {x}\\ 
=& -2 \int p({x}) \sum\limits_{i=1}^{N}(\frac{\partial \log p({x})}{\partial {x}_{i}}{s}_{\theta_i}({x})) d {x}\\ 
=& -2 \sum\limits_{i=1}^{N} \int p({x}) \frac{1}{p({x})} \frac{\partial p({x})}{\partial {x}_{i}}{s}_{\theta_i}({x}) d {x}\\ 
=& -2 \sum\limits_{i=1}^{N} \int \frac{\partial p({x})}{\partial {x}_{i}}{s}_{\theta_i}({x}) d {x}\\ 
=& 2 \sum\limits_{i=1}^{N} - \int( \frac{\partial (p({x}){s}_{\theta_i}({x}))}{\partial {x}_{i}} d {x} + \int p({x}) \frac{\partial {s}_{\theta_i}({x})}{\partial {x}_{i}}) d {x}
\end{align*}
$$
上面的最后一步是分段求积分公式，因为 $p(x)$ 取极限是0，这是PDF的特性。可得下面左边那部分为0。
$$
\begin{align*}
=& 2 - (p({x}){s}_{\theta_i}({x})\bigg\rvert^{\infty}_{-\infty}) + \sum\limits_{i=1}^{N}\int p({x}) \frac{\partial {s}_{\theta i}({x})}{\partial {x}_{i}} d {x}\\ 
=& 2 \sum\limits_{i=1}^{N} \int p({x}) \frac{\partial {s}_{\theta i}({x})}{\partial {x}_{i}} d {x}\\ 
=& 2\int p({x}) \sum\limits_{i=1}^{N} \frac{\partial {s}_{\theta i}({x})}{\partial {x}_{i}} d {x}\\ 
=& 2\int p({x}) \text{tr}(\nabla _{{x}}{s}_{\theta}({x})) d {x}
\end{align*}
$$
$tr(.)$ 函数表示矩阵的迹(trace)，即矩阵主对角线元素的总和。因为 $\nabla _{{x}}{s}_{\theta}({x})$ 是个二阶海森矩阵求导，这里只用主对角线就行。
所以最后的loss是第二和第三项的和，因为方便操作，论文中的Loss公式乘上了1/2，结果不影响：
$$\begin{align*} \frac{1}{2}\mathcal{L} &= \frac{1}{2}\int p({x}) ||{s} _{\theta}({x})||^{2} d {x} + \int p({x}) \text{tr}(\nabla _{{x}}{s}_{\theta}({x})) d {x}\\\\ &= \mathbb{E}_{p({x})}[||\frac{1}{2}{s} _{\theta}({x})||^{2} + \text{tr}(\nabla _{{x}}{s}_{\theta}({x}))] \tag{2.15} \end{align*}$$
公式似乎越来越复杂 $tr(.)$ 到底是怎么求？作者在噪声条件分数网络 NCSN 论文[3]中给出了一种解决方案。具体做法就是对原始数据加噪，使得其结果满足我们预先定义好的分布，比如高斯分布。这样，我们就知道了现在的概率密度了，于是就可以进行训练。具体操作我们接下来介绍。
### 2.2.5  噪声条件分数网络（Noise Conditional Score Networks，NCSN）
去噪分数匹配（Denoising Score Matching，DSM）方法是作者在噪声条件分数网络 NCSN 中默认的计算方法[3]。当前的问题是：不知道**原始数据分布的梯度向量**。有一种解题方法是从标准高斯噪声中构建新分布。我们考虑从标准高斯分布 $\mathcal{N}(0,𝐼)$ 中采样随机噪声𝜖，然后乘上我们预定义的 $𝜎$ ，接着加到样本 $𝑥$ 中，从而就能得到加噪后的样本 $\tilde{𝑥}$。我们构建新分布为：
$$\begin{align}
q_{\sigma}(\tilde{x}) &=\int q_σ(\tilde{x}|x) p_{data}(x) dx \tag{2.16} \\
& = \int \mathcal{N}(\tilde{x}|x,σ^2I)p_{data}(x) dx \tag{2.16+}\\
\text{and} ~ \tilde{x}&=x+\sigma\epsilon, ~ \epsilon\sim \mathcal{N}(0,I) \tag{2.17} \\
\end{align}$$公式（2.16）的积分是代表采样，有些地方会看到公式（2.16+）的形式。$p_{data}(x)$ 代表需要扰动的数据。$q_σ(\tilde{x}|x) \sim \mathcal{N}(\tilde{x}|x,σ^2I)$ 表示扰动噪声数据分布。$q_{\sigma}(\tilde{x})$ 就是我们构建的新分布， $\tilde{𝑥}$就是我们加噪的新样本。
NCSN的目标是训练一个条件分数网络 $s_{\theta}( \tilde{x},\sigma )$ 来估计扰动数据的分布，即 
$$
\frac{1}{2}\mathbb{E}_{q_σ(\tilde{x})}
\Big[\Vert s_{\theta}( \tilde{x},\sigma ) - \nabla _{\tilde{x}} \log q_σ(\tilde{x})\Vert_2^2\Big] \tag{2.18}
$$
在DSM论文[4]中，我们得到相应的推导公式
$$
\begin{align}
& ~~~~ \frac{1}{2}\mathbb{E}_{q_σ(\tilde{x})}
\Big[\Vert s_{\theta}( \tilde{x},\sigma ) - \nabla _{\tilde{x}} \log q_σ(\tilde{x})\Vert_2^2\Big] \tag{2.19} \\
&=\frac{1}{2}\int {q_σ(\tilde{x})}
\Big[\Vert s_{\theta}( \tilde{x},\sigma ) - \frac{1}{q_σ(\tilde{x})}\frac{\partial q_σ(\tilde{x})}{\partial \tilde{x}} \Vert_2^2\Big] d\tilde{x}\\
&=\frac{1}{2}\int 
\Big[\Vert {q_σ(\tilde{x})}s_{\theta}( \tilde{x},\sigma ) - \frac{\partial q_σ(\tilde{x})}{\partial \tilde{x}} \Vert_2^2\Big] d\tilde{x}\\
&=\frac{1}{2}\int 
\Big[\Vert {q_σ(\tilde{x})}s_{\theta}( \tilde{x},\sigma ) - \frac{\partial \int q_σ(\tilde{x}|x) p_{data}(x) dx}{\partial \tilde{x}} \Vert_2^2\Big] d\tilde{x} \\
&=\frac{1}{2}\int 
\Big[\Vert {q_σ(\tilde{x})}s_{\theta}( \tilde{x},\sigma ) - \int \frac{\partial q_σ(\tilde{x}|x)}{\partial \tilde{x}} p_{data}(x) dx \Vert_2^2\Big] d\tilde{x} \\
&=\frac{1}{2}\int 
\Big[\Vert {\int q_σ(\tilde{x}|x) p_{data}(x) dx} \cdot s_{\theta}( \tilde{x},\sigma ) - \int q_σ(\tilde{x}|x) p_{data}(x)\frac{\partial \log q_σ(\tilde{x}|x)}{\partial \tilde{x}} dx \Vert_2^2\Big] d\tilde{x} \\
&=\frac{1}{2}\int_{\tilde{x}} \int_x q_σ(\tilde{x}|x) p_{data}(x) 
\Big[\Vert s_{\theta}( \tilde{x},\sigma ) - \frac{\partial \log q_σ(\tilde{x}|x)}{\partial \tilde{x}}\Vert_2^2\Big] dx d\tilde{x} \\
&= \frac{1}{2}\mathbb{E}_{q_σ(\tilde{x}|x) p_{data}(x) }
\Big[\Vert s_{\theta}( \tilde{x},\sigma ) - \nabla _{\tilde{x}} \log q_σ(\tilde{x}|x)\Vert_2^2\Big] \tag{2.20}
\end{align}
$$
***
注意：宋飏博士在论文与blog有不同的标记方法。论文更准确用的是 $q_{\sigma}(x)$ ，代表与 $p(x)$ 不同；blog上用的是 $p_{\sigma}(x)$ ，为了从公式上直接替换 $p(x)$ 。
***
这里面还有一个潜在推理。当 $x$ 是 $D$ 维，即 ${x}\in \mathbb{R} ^D$ 时，由多维高斯分布 $f( \mathbf{x} ) = \frac{1}{(2\pi)^{\frac{D}{2}} | Σ |^{\frac{1}{2}}} e^{- \frac{1}{2}( x - μ )^T Σ^{-1}(x - μ)}$  可得:
$$
\begin{align}
\nabla _{\tilde{x}} \log q_σ(\tilde{x}|x) &= \nabla _{\tilde{x}}(C - \frac{1}{2}(\tilde{x} - x )^T Σ^{-1}(\tilde{x} - x)) \\
&= -Σ^{-1}(\tilde{x} - x) \\
&= -\begin{bmatrix} 𝜎_{1}^2 & & & \\ & 𝜎_{2}^2 & & \\ & & \ddots &\\ & & &𝜎_{d}^2 \end{bmatrix} (\tilde{x} - x) \tag{2.21}\\
&= - \frac{\tilde{x} - x}{𝜎^2} \tag{2.22} \\
&= - \frac{\epsilon}{𝜎} \tag{2.23}
\end{align}
$$
将（2.22）带入公式（2.20）可以写为如下形式：
$$
\begin{align}
& ~~~~ \frac{1}{2}\mathbb{E}_{q_σ(\tilde{x}|x) p_{data}(x) }
\Big[\Vert s_{\theta}( \tilde{x},\sigma ) - \nabla _{\tilde{x}} \log q_σ(\tilde{x}|x)\Vert_2^2\Big] \\
&= \frac{1}{2}\mathbb{E}_{q_σ(\tilde{x}|x)}\mathbb{E}_{p_{data}(x)}
\Big[\Big\Vert s_{\theta}( \tilde{x},\sigma ) + \frac{\tilde{x}-x}{σ^2}\Big\Vert_2^2\Big] \\ 
&= \frac{1}{2}\mathbb{E}_{p_{data}(x)}\mathbb{E}_{\tilde{x} \sim \mathcal{N}(\tilde{x}|x,σ^2I)}
\Big[\Big\Vert s_{\theta}( \tilde{x},\sigma ) + \frac{\tilde{x}-x}{σ^2}\Big\Vert_2^2\Big] \tag{2.24} \\
& =\mathcal{l}(\theta;\sigma) \tag{2.25}
\end{align}
$$
我们把最后公式（2.24）的写成公式（2.25）方便后面使用。在公式（2.24）中，我们发现，主要的问题在于噪声力度 $\sigma$ 。噪声加的大（极端情况变成纯噪声）会导致加噪之后的数据分布严重偏离原始数据分布；反之，噪声加的小（极端情况变成0噪声）不能很好的解决前述存在的问题。为了权衡这两个方面，作者提出同时使用多个不同大小噪声的退火方案。
### 2.2.6 退火朗之万动力学采样（annealed Langevin dynamics）

有 $L$ 个递增的 $𝜎_1<𝜎_2<⋯<𝜎_L$ ，分布 $q(X)$ 与每个高斯噪声 $\sigma\epsilon \sim \mathcal{N}(0,𝜎_i^2I),i=1,2,⋯,L$ 结合成受扰动的分布。

我们发现 $\sigma$ 最够小时， $\tilde{x} \approx x$  ，。

，其中当 ${x}\in \mathbb{R} ^D$ 时 $s_{\theta}\left( {x},\sigma \right) \in \mathbb{R} ^D$ 。一般把 $s_{\theta}\left( {x};t \right)$ 称为噪声条件分数网络。


![](分数在高低区域的作用.png)
我们再回到向量场的图，这个图仅仅演示用。真实情况是图像大部位是的区域是低概率密度区域，而损失函数简单的二阶计算会出现大密度区间“妨碍”小密度区间的问题。这种情况我们会加大噪声，填充整个区域。较大的噪声显然可以覆盖低密度区域，在更多区域获得更好的评分估计，但是过度破坏了数据。如图我们可以看到边界模糊，这就是强度过大的噪声反之会干扰到原始数据的分布的表现。这种情况还会造成估计分数的误差增大，从而基于加噪后的分数使用朗之万动力学采样生成的结果也就不太符合原数据分布。相反地，噪声强度小能够获得与原数据分布较为近似的效果，但是却不能够很好地“填充”低概率密度区域。为了达到两者最佳。我们使用了多尺度噪声扰动。






[1] Jonathan Ho, Ajay Jain, Pieter Abbeel. Denoising Diffusion Probabilistic Models. arXiv preprint  arXiv:2006.11239v2.
[2] Yang Song, Jascha Sohl-Dickstein, Diederik P. Kingma, Abhishek Kumar, Stefano Ermon, Ben Poole. Score-Based Generative Modeling through Stochastic Differential Equations. arXiv:2011.13456
[3] Yang Song, and Stefano Ermon. Generative Modeling by Estimating Gradients of the Data Distribution. arXiv:1907.05600
[4] Pascal Vincent. A Connection Between Score Matching and Denoising Autoencoders