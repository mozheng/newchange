## **stable diffusion模型微调方法**

主要有 4 种方式：Dreambooth, LoRA(Low-Rank Adaptation of Large Language Models), Textual Inversion, Hypernetworks。它们的区别大致如下：

- **Textual Inversion** （也称为 Embedding），它实际上并没有修改原始的 Diffusion 模型， 而是通过深度学习找到了和你想要的形象一致的角色形象特征参数，通过这个小模型保存下来。这意味着，如果原模型里面这方面的训练缺失的，其实你很难通过嵌入让它“学会”,它并不能教会 Diffusion 模型渲染其没有见过的图像内容。
- **Dreambooth** 是对整个神经网络所有层权重进行调整，会将输入的图像训练进 Stable Diffusion 模型，它的本质是先复制了源模型，在源模型的基础上做了微调（fine tunning）并独立形成了一个新模型，在它的基本上可以做任何事情。缺点是，训练它需要大量 VRAM, 目前经过调优后可以在 16GB 显存下完成训练。
- **LoRA** 也是使用少量图片，但是它是训练单独的特定网络层的权重，是向原有的模型中插入新的网络层，这样就避免了去修改原有的模型参数，从而避免将整个模型进行拷贝的情况，同时其也优化了插入层的参数量，最终实现了一种很轻量化的模型调校方法， LoRA 生成的模型较小,训练速度快, 推理时需要 LoRA 模型+基础模型，LoRA 模型会替换基础模型的特定网络层，所以它的效果会依赖基础模型。
- **Hypernetworks** 的训练原理与 LoRA 差不多，目前其并没有官方的文档说明，与 LoRA 不同的是，Hypernetwork 是一个单独的神经网络模型，该模型用于输出可以插入到原始 Diffusion 模型的中间层。 因此通过训练，我们将得到一个新的神经网络模型，该模型能够向原始 Diffusion 模型中插入合适的中间层及对应的参数，从而使输出图像与输入指令之间产生关联关系。

总儿言之，就训练时间与实用度而言，目前训练LoRA性价比更高，也是当前主流的训练方法。

所以这里只介绍如何训练LoRA模型。