
该系列 翻译自《Understanding Diffusion Models: A Unified Perspective》Calvin Luo, Google Research, Brain Team
## 生成模型
一个样本 $x$ 从我们感兴趣的分布中观测到，**生成模型**的目的就是学习建模这个真实的数据分布 $p(x)$ 。一旦学习到 $p(x)$ ，我们就可以随意地从该分布中获取生成我们感兴趣的其他样本。此外，在一些公式下，我们也可以使用这些学习到的模型来评估“观测”或“抽样”到的其他数据的质量。
在目前的文献中有几个众所周知的方向，我们简要回顾一下：
1. 生成对抗性网络（GANs），对复杂分布的抽样过程进行建模，并以对抗性的方式进行学习。
2. 还有一种基于“似然”的生成模型，该模型试图学习一个使观察数据样本可能性最大化的模型。这包括自回归模型、规范化流和变分自动编码器（VAEs）。
3. 最后一种方案是基于”能量“的建模，这种分布被学习成一个任意的能量函数，然后被归一化。基于分数（Score-based）的生成模型就很像这种模式；但他不是学习能量函数本身，而是学习能量模型的分数，从而进行神经网络建模。
在本文中，我们探索和回顾扩散模型，正如我们将证明的那样，它有基于”似然“的解释，也有基于”分数“的解释 。我们会详细展示这些模型背后的数学原理， 目的是让任何人都可以跟踪和理解什么是扩散模型以及它们是如何工作的。

## 背景： ELBO、VAE和层次化VAE
对于许多模式，我们可以认为我们观察到的数据是由一个看不见却有联系的潜在变量（latent variable）表示或生成的，我们可以用随机变量 $z$ 来表示它。表达这一观点的最好的例子就是柏拉图的“洞穴寓言”。（如下图1，来自于维基百科）在寓言中，一群人一生都被锁在一个洞穴里，只能看到投射在他们前面一堵墙上的二维阴影。（如图1左边）这些阴影是由洞穴人看不见的三维物体经过火把前产生的。 对这些洞穴人来说，他们所观察到的一切实际上都是由他们永远无法看到的高维抽象概念所决定的。
![洞穴寓言](images/洞穴寓言.jpeg)

类似地，我们在现实世界中遇到的对象也可以作为一些高级表示的函数生成；例如，这种表示可能封装抽象属性 , 如颜色、大小、形状等。然后，我们观察到的可以被解释为这些抽象概念的三维投影或实例化，就像洞穴人们 观察到的实际上是三维物体的二维投影一样。虽然洞穴里的人永远看不到（甚至完全理解）隐藏的物体，但他们 仍然可以对它们进行推理和推断； 以类似的方式，我们可以近似地描述我们观察到的数据的潜在表征。

虽然柏拉图的寓言说明了潜在变量背后的观点，即决定观察，但这个类比需要注意的是，在生成建模中，我们通 常寻求学习低维的潜在表征，而不是高维的表征。这是因为试图学习比观察更高维度的表示，没有强先验是徒劳 的努力。另一方面，学习低维延迟也可以被视为一种压缩的形式，并可以潜在地揭示描述观察的语义上有意义的 结构。



